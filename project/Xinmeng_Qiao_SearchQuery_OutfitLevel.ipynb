{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(line, stopword_set):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in word_tokenize(line):\n",
    "        if word.lower() in stopword_set:\n",
    "            continue\n",
    "        new_words.append(word.lower())\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def word_count(lst):\n",
    "    d = {}\n",
    "    for line in lst:\n",
    "        for word in line.split():\n",
    "            d[word] = d.get(word,0) + 1\n",
    "    return d\n",
    "\n",
    "\n",
    "# https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "\n",
    "def query_cat(query):\n",
    "    outfit_dict = {}\n",
    "    for word in word_tokenize(query):\n",
    "        if word in bottom_lst:\n",
    "            outfit_dict['bottom'] = outfit_dict.get('bottom', 0) + 1\n",
    "        if word in one_piece_lst:\n",
    "            outfit_dict['onepiece'] = outfit_dict.get('onepiece', 0) + 1\n",
    "        if word in shoe_lst:\n",
    "            outfit_dict['shoe'] = outfit_dict.get('shoe', 0) + 1\n",
    "        if word in top_lst:\n",
    "            outfit_dict['top'] = outfit_dict.get('top', 0) + 1\n",
    "        if word in accessory_lst:\n",
    "            outfit_dict['accessory'] = outfit_dict.get('accessory', 0) + 1\n",
    "\n",
    "    if not outfit_dict:\n",
    "        query_cat = 'Unknown'\n",
    "    else:\n",
    "        query_d = list(sorted(outfit_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "        query_cat = query_d[0][0]\n",
    "    return query_cat\n",
    "\n",
    "def find_most_sim(query_cat, d):\n",
    "    if query_cat == 'Unknown':\n",
    "        most_sim = d[0][0]\n",
    "\n",
    "    else:\n",
    "        for i in range(len(d)-1):\n",
    "            product_idx = d[i][0]\n",
    "            cat = df_cleaned.loc[product_idx,'outfit_item_type']\n",
    "            if cat == query_cat:\n",
    "                most_sim = product_idx\n",
    "                break\n",
    "    return most_sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and merge dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xinmengqiao/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py:4147: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return super().fillna(\n",
      "<ipython-input-4-440569dd6ebc>:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['combined'] = df_final[features].apply(lambda col: ' '.join(col.astype(str)), axis=1)\n"
     ]
    }
   ],
   "source": [
    "df_outfit = pd.read_csv('outfit_combinations USC.csv')\n",
    "\n",
    "dic = {'accessory1':'accessory','accessory2':'accessory',\n",
    "       'accessory3':'accessory', 'bottom':'bottom', 'top':'top', \n",
    "       'shoe':'shoe', 'onepiece':'onepiece'}\n",
    "\n",
    "a = df_outfit['outfit_item_type'].map(dic)\n",
    "df_outfit['outfit_item_type'] = a\n",
    "\n",
    "df_product = pd.read_excel('Behold+product+data+04262021.xlsx')\n",
    "\n",
    "df_combined = df_outfit.merge(df_product, how = 'left', left_on = 'product_id', right_on = 'product_id')\n",
    "df_final = df_combined[['product_id','outfit_id','outfit_item_type',\n",
    "                        'product_full_name','description', 'details']]\n",
    "\n",
    "df_final.fillna(value='Unknown',inplace=True)\n",
    "features = ['product_full_name','details', 'description']\n",
    "df_final['combined'] = df_final[features].apply(lambda col: ' '.join(col.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for the product documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-5bfbf729fece>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['cleaned'] = X['combined'].apply(lambda x: regex.sub(' ', x))\n",
      "<ipython-input-5-5bfbf729fece>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['removed'] = a\n",
      "<ipython-input-5-5bfbf729fece>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_final['text_feature'] = b\n"
     ]
    }
   ],
   "source": [
    "# clean the text features for each product\n",
    "X = df_final[['combined']]\n",
    "regex = re.compile(r'[^\\w\\s\\d]')\n",
    "X['cleaned'] = X['combined'].apply(lambda x: regex.sub(' ', x))\n",
    "\n",
    "# remove stopwords in the text features\n",
    "stp = set(stopwords.words('english'))\n",
    "added = ['unknown','½ï']\n",
    "stp.update(added)\n",
    "a = X['cleaned'].apply(lambda x: remove_stopwords(x, stp))\n",
    "X['removed'] = a\n",
    "\n",
    "# lemmatize the features. We chose lemmatization over stemming to keep the meaning of the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "a = X['removed'].apply(lambda x: lemmatize_sentence(x))\n",
    "b = a.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# futher clean the features by using regex to remove single character words and digits\n",
    "regex = re.compile(r'\\d')\n",
    "b = b.apply(lambda x: regex.sub(' ',x))\n",
    "regex_let = re.compile(r'\\b\\w\\b')\n",
    "b = b.apply(lambda x: regex_let.sub('',x))\n",
    "df_final['text_feature'] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataframe used for all search querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cleanned dataframe that used for search query. Since this dataframe is identical to all querys, \n",
    "# so we did not include data cleaning before in our search query function to save run time \n",
    "df_cleaned = df_final[['outfit_id','product_id','text_feature','outfit_item_type']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict outfit category for query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_lst = ['pant', 'skirt', 'jean', 'trouser', 'short', 'tight', 'legging', 'culotte',\n",
    "              'bootcut', 'leg', 'palazzo', 'bottom', 'ankle', 'capri', 'waistband',\n",
    "              'waist', 'highwaist', 'beltless', 'straight']\n",
    "\n",
    "one_piece_lst = ['dress', 'piece', 'jumpsuit', 'piece', 'bodysuit', 'slipdress', 'shirtdress',\n",
    "                 'onepiece', 'onesie', 'gown', 'robe', 'romper', 'shortall', 'minidress',\n",
    "                 'caftan', 'tunic', 'georgette', 'coverall', 'bikini', 'kimono', 'sundress',\n",
    "                 'boilersuit']\n",
    "\n",
    "shoe_lst = ['shoe', 'boot', 'sneaker', 'heel', 'foot', 'sandal', 'slipper', \n",
    "            'flip', 'flop', 'bootie', 'toe', 'pump', 'trainer', 'platform', 'oxford', \n",
    "            'mule', 'brogue', 'loafer', 'moccasin', 'flat', 'derby', 'slingback', 'clog',\n",
    "            'heighten', 'shoetie']\n",
    "\n",
    "top_lst = ['tank', 'top', 'blouse', 'shirt', 'tee', 'vest', 'blazer', 'crop', \n",
    "           'hoodie', 'hood', 'sweat', 'turtleneck', 'cardigan', 'camisole', 'sweatshirt', \n",
    "           'neck', 'sleeve', 'cami', 'boatneck', 'vneck', 'jersey', 'sleeves', 'tanktop',\n",
    "           'sweatpants', 'bustier', 'sleeveless', 'neckline', 'crewneck', 'longsleeve']\n",
    "\n",
    "accessory_lst = ['satchel', 'clutch', 'bag', 'tote', 'jacket', 'coat', 'scarf', \n",
    "                 'bra', 'bralette', 'backpack', 'briefcase', 'purse', 'panty', \n",
    "                 'thong', 'belt', 'hat', 'bralett', 'hobo', 'eye', 'sunglasses', 'bib', \n",
    "                 'accessory', 'sunglass', 'lens', 'trench', 'wallet', 'earring', 'barrette',\n",
    "                 'pullover', 'photo', 'card', 'band', 'felt', 'hand', 'necklace',\n",
    "                 'shearling', 'cream', 'lip', 'balm', 'parka', 'mask', 'bracelet',\n",
    "                 'sock', 'glasswear', 'cape', 'suit', 'bandana', 'lenses', 'lingerie',\n",
    "                 'collar', 'apron', 'tie', 'strap', 'ring', 'napkin', 'shawl', 'sweater', \n",
    "                 'beret', 'sapphire', 'crossbody', 'neckband', 'headband', 'headgear',\n",
    "                 'outerwear', 'wrist', 'cap', 'shirtjacket', 'windbreaker', 'glove', 'mitt',\n",
    "                 'bangle', 'obi', 'stud', 'earing', 'overcoat', 'trenchcoat', 'watch', 'anklet',\n",
    "                 'mitts', 'choker', 'pin', 'gloves']\n",
    "\n",
    "others_lst = ['towel', 'vase', 'chair', 'candle', 'photo', 'card', 'book', 'lamp', \n",
    "              'pottery', 'plate', 'salt', 'pillow', 'table', 'bench', 'bed', 'table',\n",
    "              'couch', 'baby', 'basket', 'crochet', 'coverlet', 'upholster', 'cushion',\n",
    "              'makeup', 'ceramic', 'soap', 'antique', 'sofa', 'footbed', 'goblet', 'skateboard',\n",
    "              'quilt', 'washcloth', 'comb', 'fragrance', 'mat', 'swimwear', 'swimsuit', 'wetsuit',\n",
    "              'pillowcase', 'perfume', 'enamel', 'insole', 'shower', 'furniture', 'toiletry',\n",
    "              'pilowcases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(user_query: str):\n",
    "    # perform datacleaning for the input query\n",
    "    regex = re.compile(r'[^\\w\\s\\d]')\n",
    "    query = regex.sub(' ', user_query)\n",
    "    lst = []\n",
    "    for word in word_tokenize(query):\n",
    "        if word not in stp:\n",
    "            lst.append(word)\n",
    "    query = ' '.join(lst)\n",
    "    query_lst = lemmatize_sentence(query)\n",
    "    query = ' '.join(query_lst)\n",
    "    query_df = pd.DataFrame({'product_id':['query'],'text_feature':[query]},index=[5291])\n",
    "    df_withquery = pd.concat([df_cleaned,query_df])\n",
    "    \n",
    "    # use TF-IDF to vectorize the product table, max_features is set for 1000 to avoid noise\n",
    "    products = df_withquery['text_feature'].values\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(products)\n",
    "    tf_idf_lookup_table = pd.DataFrame(X.toarray(), columns= vectorizer.get_feature_names(), index=df_withquery.index)\n",
    "    \n",
    "    DOCUMENT_SUM_COLUMN = \"DOCUMENT_TF_IDF_SUM\"\n",
    "\n",
    "    # sum the tf idf scores for each document\n",
    "    tf_idf_lookup_table[DOCUMENT_SUM_COLUMN] = tf_idf_lookup_table.sum(axis=1)\n",
    "    available_tf_idf_scores = tf_idf_lookup_table.columns # a list of all the columns we have\n",
    "    available_tf_idf_scores = set(map( lambda x: x.lower(), available_tf_idf_scores)) # lowercase everything\n",
    "    \n",
    "    products_vectors = []\n",
    "    \n",
    "    for idx, product in enumerate(products): # iterate through each review\n",
    "        tokens = nlp(product) # have spacy tokenize the review text\n",
    "\n",
    "        # initially start a running total of tf-idf scores for a document\n",
    "        total_tf_idf_score_per_document = 0\n",
    "        # start a running total of initially all zeroes (300 is picked since that is the word embedding size used by word2vec)\n",
    "        running_total_word_embedding = np.zeros(300) \n",
    "        for token in tokens: # iterate through each token\n",
    "        # if the token has a pretrained word embedding it also has a tf-idf score\n",
    "            if token.has_vector and token.text.lower() in available_tf_idf_scores:\n",
    "                tf_idf_score = tf_idf_lookup_table.loc[idx, token.text.lower()]\n",
    "                running_total_word_embedding += tf_idf_score * token.vector\n",
    "                total_tf_idf_score_per_document += tf_idf_score\n",
    "                \n",
    "        # divide the total embedding by the total tf-idf score for each document\n",
    "        # print(total_tf_idf_score_per_document)\n",
    "       \n",
    "        document_embedding = running_total_word_embedding / max(1,total_tf_idf_score_per_document)\n",
    "        products_vectors.append(document_embedding.tolist())\n",
    "\n",
    "    # generate a dictionary contains the similarity between query and each product \n",
    "    query_vec = products_vectors[-1]\n",
    "    similarity_lst = {}\n",
    "    for i in range(len(products_vectors)-1):\n",
    "        product = products_vectors[i]\n",
    "        if (norm(product)*norm(query_vec)) == 0:\n",
    "            similarity_lst[i] = 0\n",
    "            continue\n",
    "        cos_sim = dot(product, query_vec)/(norm(product)*norm(query_vec))\n",
    "        similarity_lst[i] = cos_sim\n",
    "\n",
    "    d = list(sorted(similarity_lst.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # before matching the product, we first use domain knowledge to determine which category of clothes the user is trying to search for\n",
    "    cat_find = query_cat(query)\n",
    "    # Based on the category, we return the product with highest similarity\n",
    "    found_idx = find_most_sim(cat_find, d)\n",
    "    re_outfit_id = df_final.iloc[found_idx]['outfit_id']\n",
    "    # Then, we return the whole outfit which contain the product selected in last step\n",
    "    returned_df = df_final[df_final['outfit_id'] == re_outfit_id][['outfit_item_type', 'product_id','product_full_name']]\n",
    "    returned_df = returned_df.set_index('outfit_item_type')\n",
    "    final_dict = returned_df.groupby(level=0).apply(lambda x: x.to_dict('r')).to_dict()\n",
    "    return final_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accessory': [{'product_id': '01DVPMF2X9M22VBARKC5G6FXB7',\n",
       "   'product_full_name': 'Le Riviera leather shoulder bag'},\n",
       "  {'product_id': '01DVPNB5C3973WN8R7W7YVRRKT', 'product_full_name': '#NAME?'},\n",
       "  {'product_id': '01DVPNB5C3973WN8R7W7YVRRKT',\n",
       "   'product_full_name': '+ Pernille Teisbaek Clara oversized belted faux fur coat'}],\n",
       " 'bottom': [{'product_id': '01DVMERT64RSJQBBJS9973N7HF',\n",
       "   'product_full_name': 'Femme Hi Spikes high-rise straight-leg jeans'}],\n",
       " 'shoe': [{'product_id': '01DVCTFR5MA1ZDKTAFS4VG4VW4',\n",
       "   'product_full_name': 'Cabria leather ankle boots'}],\n",
       " 'top': [{'product_id': '01DT50PZ3D0RXNZFDSGTWMVXZW',\n",
       "   'product_full_name': 'Boy striped cotton-jersey T-shirt'}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Query\n",
    "query = 'large size, straight leg pant with a white dot'\n",
    "dic = search(query)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accessory': [{'product_id': '01DPEHS0XH9PDD1GH5ZE4P43A2',\n",
       "   'product_full_name': 'Cassi Belt Bag'},\n",
       "  {'product_id': '01DPGV0TFFJ720BT3F8ADN4V7P',\n",
       "   'product_full_name': \"Women's 2011 Icon trench\"}],\n",
       " 'onepiece': [{'product_id': '01DPD4R5X5TQCWTVTC2AEAFC10',\n",
       "   'product_full_name': 'Ida Dress'}],\n",
       " 'shoe': [{'product_id': '01DPKNCMSFAWF2HVQSRHHXDV0K',\n",
       "   'product_full_name': 'Virginia Boot'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Query\n",
    "query = 'yellow onepiece for beach with pink flower'\n",
    "dic = search(query)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bottom': [{'product_id': '01E223GQFQFHBZR1106AE2VKJ3',\n",
       "   'product_full_name': 'Wide Leg Ankle Trousers'}],\n",
       " 'shoe': [{'product_id': '01E1JM43NQ3H17PB22EV3074NX',\n",
       "   'product_full_name': 'Visa Mule'}],\n",
       " 'top': [{'product_id': '01E223E4WZNM9BW7A6XCQMJ965',\n",
       "   'product_full_name': 'Silk Button-Up Shirt'}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example Query\n",
    "query = 'slim fitting, straight leg pant with a center back zipper and slightly cropped leg'\n",
    "dic = search(query)\n",
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
