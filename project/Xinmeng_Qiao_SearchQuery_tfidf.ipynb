{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import spacy\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(line, stopword_set):\n",
    "    new_words = []\n",
    "    \n",
    "    for word in word_tokenize(line):\n",
    "        if word.lower() in stopword_set:\n",
    "            continue\n",
    "        new_words.append(word.lower())\n",
    "    \n",
    "    return ' '.join(new_words)\n",
    "\n",
    "def word_count(lst):\n",
    "    d = {}\n",
    "    for line in lst:\n",
    "        for word in line.split():\n",
    "            d[word] = d.get(word,0) + 1\n",
    "    return d\n",
    "\n",
    "\n",
    "# https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cat(df, col, idx_lst):\n",
    "    for i in idx_lst:\n",
    "        string = df.loc[i,col]\n",
    "        if regex_bottom.search(string):\n",
    "            df.loc[i,'product_category'] = 'bottom'\n",
    "        elif regex_shoe.search(string):\n",
    "            df.loc[i,'product_category'] = 'shoes'\n",
    "        elif regex_accessory.search(string):\n",
    "            df.loc[i,'product_category'] = 'accessory'\n",
    "        elif regex_top.search(string):\n",
    "            df.loc[i,'product_category'] = 'top'\n",
    "        else:\n",
    "            df.loc[i,'product_category'] = 'other'\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning for the product documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the excel for products, replace null values with word 'unknown', combined features details and description together\n",
    "df = pd.read_excel('Behold+product+data+04262021.xlsx')\n",
    "df.fillna(value='Unknown',inplace=True)\n",
    "features = ['details', 'description']\n",
    "df['combined'] = df[features].apply(lambda col: ' '.join(col.astype(str)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each product with a product category using regex\n",
    "regex = re.compile(r'[^\\w\\s\\d]')\n",
    "df['brand_cat'] = df['brand_category'].apply(lambda x: regex.sub(' ', x))\n",
    "\n",
    "bottoms = r'(?i)\\b(pants?|jeans?|skirts?|shorts?|bottoms?|trousers?|legs|leggings)\\b'\n",
    "shoes = r'(?i)\\b(shoes?|heels?|sandals?|wedges?|boots?|booties?|uggs?|flats?|skates|flip-flops?|brogues?|skates?|jackboots?|sneakers?|slippers?)\\b'\n",
    "accessory = r'(?i)\\b(case|scarf|scarves|handbags?|purse|clutch|bags?|muffs?|wristlets?|baguettes?|totes?|backpacks?|hats?|masks?|jewelry|earrings?|necklaces?|rings?|watch|bracelets?)\\b'\n",
    "top = r'(?i)\\b(shirts?|coats?|jackets?|sweaters?|tops?|collars?|sweatshirts?|sleeves?|tshirts?|tanks?)\\b'\n",
    "\n",
    "regex_bottom = re.compile(bottoms)\n",
    "regex_shoe = re.compile(shoes)\n",
    "regex_accessory = re.compile(accessory)\n",
    "regex_top = re.compile(top)\n",
    "\n",
    "# first assign category based on the brand_category feature\n",
    "all_idx = list(df.index)\n",
    "df_withcat = find_cat(df, 'brand_cat', all_idx)\n",
    "# if brand_category features did not contain useful information, assign category using details and description\n",
    "other_idx = list(df_withcat[df_withcat['product_category']=='other'].index)\n",
    "df_withcat = find_cat(df_withcat, 'combined', other_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-373008cd6867>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['cleaned'] = X['combined'].apply(lambda x: regex.sub(' ', x))\n",
      "<ipython-input-6-373008cd6867>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X['removed'] = a\n"
     ]
    }
   ],
   "source": [
    "# clean the text features for each product\n",
    "X = df[['combined']]\n",
    "regex = re.compile(r'[^\\w\\s\\d]')\n",
    "X['cleaned'] = X['combined'].apply(lambda x: regex.sub(' ', x))\n",
    "\n",
    "# remove stopwords in the text features\n",
    "stp = set(stopwords.words('english'))\n",
    "added = ['unknown','½ï']\n",
    "stp.update(added)\n",
    "a = X['cleaned'].apply(lambda x: remove_stopwords(x, stp))\n",
    "X['removed'] = a\n",
    "\n",
    "# lemmatize the features. We chose lemmatization over stemming to keep the meaning of the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "a = X['removed'].apply(lambda x: lemmatize_sentence(x))\n",
    "b = a.apply(lambda x: ' '.join(x))\n",
    "\n",
    "# futher clean the features by using regex to remove single character words and digits\n",
    "regex = re.compile(r'\\d')\n",
    "b = b.apply(lambda x: regex.sub(' ',x))\n",
    "regex_let = re.compile(r'\\b\\w\\b')\n",
    "b = b.apply(lambda x: regex_let.sub('',x))\n",
    "df['text_feature'] = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final dataframe used for all search querys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the cleanned dataframe that used for search query. Since this dataframe is identical to all querys, \n",
    "# so we did not include data cleaning before in our search query function to save run time \n",
    "df_cleaned = df[['product_id','text_feature','product_category']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search query function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(user_query: str):\n",
    "    # perform datacleaning for the input query\n",
    "    regex = re.compile(r'[^\\w\\s\\d]')\n",
    "    query = regex.sub(' ', user_query)\n",
    "    query_df = pd.DataFrame({'product_id':['query'],'text_feature':[query]},index=[61355])\n",
    "    df_withquery = pd.concat([df_cleaned,query_df])\n",
    "    \n",
    "    # use TF-IDF to vectorize the product table, max_features is set for 1000 to avoid noise\n",
    "    products = df_withquery['text_feature'].values\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(products)\n",
    "    tfidf_df = pd.DataFrame(X.toarray(), columns= vectorizer.get_feature_names(), index=df_withquery.index)\n",
    "    \n",
    "    # calculate the tf-idf vector for the input query\n",
    "    query_tfidf = tfidf_df.iloc[61355]\n",
    "    \n",
    "    # generate a simialrity table to reflect similarity between the input query and all avaliable products\n",
    "    similarity_lst = {}\n",
    "    for i in range(len(tfidf_df)-1):\n",
    "        product = tfidf_df.iloc[i]\n",
    "        if (norm(product)*norm(query_tfidf)) == 0:\n",
    "            similarity_lst[i] = 0\n",
    "            continue\n",
    "        cos_sim = dot(product, query_tfidf)/(norm(product)*norm(query_tfidf))\n",
    "        similarity_lst[i] = cos_sim\n",
    "    d = list(sorted(similarity_lst.items(), key=lambda item: item[1], reverse=True))\n",
    "    \n",
    "    # for each category, we return one product with highest cosine similarity. \n",
    "    bottom = 'Unknow'\n",
    "    shoes = 'Unknow'\n",
    "    accessory = 'Unknow'\n",
    "    top = 'Unknow'\n",
    "    \n",
    "    for i in range(10000):\n",
    "        product_idx = d[i][0]\n",
    "        cat = df_cleaned.loc[product_idx,'product_category']\n",
    "        if bottom == 'Unknow' and cat =='bottom':\n",
    "            bottom = (df.loc[product_idx,'name'], df.loc[product_idx,'product_id'])\n",
    "        if shoes == 'Unknow' and cat =='shoes':\n",
    "            shoes = (df.loc[product_idx,'name'], df.loc[product_idx,'product_id'])\n",
    "        if accessory== 'Unknow' and cat =='accessory':\n",
    "            accessory = (df.loc[product_idx,'name'], df.loc[product_idx,'product_id'])\n",
    "        if top== 'Unknow' and cat =='top':\n",
    "            top = (df.loc[product_idx,'name'], df.loc[product_idx,'product_id'])\n",
    "        if (bottom != 'Unknow') and (shoes != 'Unknow') and (accessory != 'Unknow') and (top != 'Unknow'):\n",
    "            break\n",
    "    \n",
    "    result_dic = {'bottom':bottom, 'shoes':shoes, 'accessory':accessory, 'top':top}\n",
    "    return result_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bottom': ('Asymmetric Skirt', '01EMPK82BDK6E6S4KBRP77RRAC'),\n",
       " 'shoes': ('Ikat Dot  V-neck caftan', '01EWC89HRBR5N9Z6JNKGFAZAD2'),\n",
       " 'accessory': ('Musgrave', '01EEBHSPQXAN74785CGB7GKPZS'),\n",
       " 'top': ('Dot Top', '01ECAZH7666P4KPPFGPV3PE7FB')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example query and results\n",
    "query = 'black slim jean with white dot'\n",
    "dic = search(query)\n",
    "dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
