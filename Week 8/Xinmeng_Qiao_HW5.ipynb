{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5 (Due Friday, 11:59pm PST, May 7th)\n",
    "\n",
    "This homework is **optional, and worth 6 points**. These **six points will be added to your overall final homework average**. Any leftover points will be added to your midterm grade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Build A Classification Model w/ Amazon\n",
    "\n",
    "Build a classification model using the **Amazon toy reviews dataset** that is able to predict on a hold-out set the sentiment of the reviews with at minimum 91% accuracy (do not round).\n",
    "\n",
    "You may incorporate as many samples as you wish (out of the original ~120,000) data points. However, **the class balance in your training and test set must be 50/50**.\n",
    "\n",
    "You will likely need to include some preprocessing techniques that we have learned about so far in this course.\n",
    "\n",
    "If you are unable to achieve 91% accuracy, then please show in this notebook at least **3 different models** that you have tried (ie. RNN, LSTM using `word2vec`, `GloVe`, `doc2vec`, etc.)\n",
    "\n",
    "**Make sure to cite your sources if you use other people's code or ideas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from numpy import array, argmax, asarray, zeros\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.recurrent import SimpleRNN, LSTM\n",
    "from keras.layers import Flatten, Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gaurav5430.medium.com/using-nltk-for-lemmatizing-sentences-c1bfff963258\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_reviews = open(\"good_amazon_toy_reviews.txt\").readlines()\n",
    "bad_reviews = open(\"poor_amazon_toy_reviews.txt\").readlines()\n",
    "\n",
    "num = 4000\n",
    "sampled_good_reviews = good_reviews[:num]\n",
    "sampled_bad_reviews = bad_reviews[:num]\n",
    "\n",
    "reviews = sampled_good_reviews + sampled_bad_reviews\n",
    "labels = np.concatenate([np.ones(num), np.zeros(num)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove punctuation and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = re.compile(r'&#[0-9]{2,3};')\n",
    "regex2 = re.compile(r'<br />')\n",
    "regex3 = re.compile(r'[^\\w\\s\\d]')\n",
    "reviews_re = []\n",
    "for review in reviews:\n",
    "    newline = regex.sub('',review)\n",
    "    newline = regex2.sub('',newline)\n",
    "    newline = regex3.sub('',newline)\n",
    "    reviews_re.append(newline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "reviews_clean = []\n",
    "for line in reviews_re:\n",
    "    newline = []\n",
    "    for word in word_tokenize(line):\n",
    "        if word.lower() in stopword_set:\n",
    "            continue\n",
    "        else:\n",
    "            newline.append(word.lower())\n",
    "    review = ' '.join(newline)\n",
    "    reviews_clean.append(review)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_lemma = [' '.join(lemmatize_sentence(line)) for line in reviews_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num = 6000\n",
    "tokenizer = Tokenizer(num_words=max_num, oov_token=\"UNKNOWN_TOKEN\")\n",
    "tokenizer.fit_on_texts(reviews_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_docs = tokenizer.texts_to_sequences(reviews_lemma)\n",
    "# number represents the index position of the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 64\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=MAX_SEQUENCE_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode label and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "labels = to_categorical(encoder.fit_transform(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_docs, labels, test_size=0.2, stratify = labels, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "def load_glove_vectors():\n",
    "    embeddings_index = {}\n",
    "    with open('glove.6B.100d.txt') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "embeddings_index = load_glove_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = int(len(tokenizer.word_index) * 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = zeros((VOCAB_SIZE, 100))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_binary_classification_rnn_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(SimpleRNN(units=64, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "def make_lstm_classification_model(plot=False):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(VOCAB_SIZE, 100, weights=[embedding_matrix], input_length=MAX_SEQUENCE_LENGTH, trainable=False))\n",
    "    model.add(Masking(mask_value=0.0)) # masking layer, masks any words that don't have an embedding as 0s.\n",
    "    model.add(LSTM(units=32, input_shape=(1, MAX_SEQUENCE_LENGTH)))\n",
    "    model.add(Dense(16))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "    optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # summarize the model\n",
    "    model.summary()\n",
    "    \n",
    "    if plot:\n",
    "        plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 64, 100)           1155600   \n",
      "_________________________________________________________________\n",
      "masking (Masking)            (None, 64, 100)           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                17024     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,173,186\n",
      "Trainable params: 17,586\n",
      "Non-trainable params: 1,155,600\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = make_lstm_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "198/198 [==============================] - 5s 16ms/step - loss: 0.4787 - accuracy: 0.7764 - val_loss: 0.2889 - val_accuracy: 0.9062\n",
      "Epoch 2/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.2599 - accuracy: 0.8952 - val_loss: 0.2613 - val_accuracy: 0.9219\n",
      "Epoch 3/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.2396 - accuracy: 0.8981 - val_loss: 0.2715 - val_accuracy: 0.9219\n",
      "Epoch 4/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.2045 - accuracy: 0.9209 - val_loss: 0.2482 - val_accuracy: 0.9219\n",
      "Epoch 5/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.2000 - accuracy: 0.9192 - val_loss: 0.2732 - val_accuracy: 0.8906\n",
      "Epoch 6/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.1768 - accuracy: 0.9295 - val_loss: 0.2864 - val_accuracy: 0.8594\n",
      "Epoch 7/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.1487 - accuracy: 0.9423 - val_loss: 0.3395 - val_accuracy: 0.8438\n",
      "Epoch 8/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.1313 - accuracy: 0.9489 - val_loss: 0.2589 - val_accuracy: 0.8906\n",
      "Epoch 9/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.1238 - accuracy: 0.9584 - val_loss: 0.3289 - val_accuracy: 0.8750\n",
      "Epoch 10/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0950 - accuracy: 0.9699 - val_loss: 0.3491 - val_accuracy: 0.8281\n",
      "Epoch 11/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0896 - accuracy: 0.9689 - val_loss: 0.3052 - val_accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0806 - accuracy: 0.9727 - val_loss: 0.4300 - val_accuracy: 0.8281\n",
      "Epoch 13/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0731 - accuracy: 0.9751 - val_loss: 0.3690 - val_accuracy: 0.8906\n",
      "Epoch 14/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0604 - accuracy: 0.9803 - val_loss: 0.4240 - val_accuracy: 0.8750\n",
      "Epoch 15/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0472 - accuracy: 0.9863 - val_loss: 0.5006 - val_accuracy: 0.8750\n",
      "Epoch 16/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0733 - accuracy: 0.9769 - val_loss: 0.5913 - val_accuracy: 0.8281\n",
      "Epoch 17/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0392 - accuracy: 0.9896 - val_loss: 0.7273 - val_accuracy: 0.8125\n",
      "Epoch 18/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0606 - accuracy: 0.9779 - val_loss: 0.5028 - val_accuracy: 0.8906\n",
      "Epoch 19/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0290 - accuracy: 0.9903 - val_loss: 0.6232 - val_accuracy: 0.8906\n",
      "Epoch 20/20\n",
      "198/198 [==============================] - 2s 12ms/step - loss: 0.0335 - accuracy: 0.9885 - val_loss: 0.6854 - val_accuracy: 0.8750\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2)\n",
    "history = model.fit(X_train, y_train,validation_split = 0.01, epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 4ms/step - loss: 0.4725 - accuracy: 0.9119\n",
      "Accuracy: 91.187501\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model final accuracy 91.19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Build A Multi-Class Classification Model w/ BBC News Dataset\n",
    "\n",
    "Perform the same classification exercise using the `bbc-text.csv` dataset. There are 5 distinct categories. You must achieve a baseline accuracy of at least 61% on a hold-out test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to set the random seeds in your notebook so I can run your results and get the same exact output:\n",
    "\n",
    "```python\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(32)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
